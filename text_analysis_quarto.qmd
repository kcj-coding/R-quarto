---
title: "Text analysis example"
format: html
toc: true
toc-location: left
editor: visual
date: "`r Sys.Date()`"
self-contained: true # true so that only 1 file is generated (not folder of files)
---

```{r setup, include=FALSE}
#knitr::opts_chunk$set(echo = F, message = F)
#library(GGally)
library(ggraph)
library(igraph)
library(Matrix)
#library(network)
library(tidyverse)
library(dplyr)
library(readxl)
library(quanteda)
library(tm)
library(vader)
library(topicmodels)
library(tidytext)
library(lexRankr)
library(DT)
library(wordcloud)
library(plotly)
library(reshape2)
#library(quarto)
#library(forcats)
#library(corpus)
```

```{r code,include=FALSE, echo=FALSE}

# load data csv, xlsx or text
text <- "The story began in 1820 when scheduled year-round Steam Packet Services started operating between Southampton, Cowes, Ryde, Southsea and Portsmouth Harbour. Excursion Services were also a mainstay activity during those balmy Victorian and Edwardian summers with daily trips along the South Coast and across the channel to France. In the 1900's, the growing interest in travel provided the Company with an important string to its bow - the provision of tendering services to the great passenger liners.

Many would anchor in Cowes Roads and use Red Funnel's steamers for alighting passengers and transferring all kinds of cargo. In 1885 the Company expanded into Tug Ownership and continued to consolidate its market position throughout the wars. After World War II, recession and changing holiday habits led to the decline of the excursion business and preoccupation with the Southampton-Cowes packet services, particularly in light of growing car ownership and the transfer of cargo onto wheels. Diesel propulsion marked a turning point in the Company's history with the first purpose-built diesel vehicle ferries joining the fleet in the 1950's.

Drawing from the Company's archives, this microsite offers a nostalgic insight into cross-Solent travel since the 1820's. It's rare for a firm to have survived for so long, and the Company is rightly proud of its illustrious history and grateful to those who've played a part

Red Funnel was formed in 1861 as the ‘Original Isle of Wight ferry operator’. Today, we carry 3.4M passengers and 860,000 vehicles across the Solent each year. The Company operates 365 days a year, with 30,000 sailings between Southampton on the UK mainland and East and West Cowes on the Isle of Wight. Our fleet consists of modern purpose-built Ro-Pax vehicle ferries and Red Jet Hi-Speed passenger catamarans. Catering and retail services are provided in-house.

We are proud to offer our customers award-winning ferry travel to the Isle of Wight. If you’re looking to take a UK break somewhere beautiful, we’ll sail you from Southampton to the Isle of Wight in under an hour with a vehicle, or 30 minutes if you’re travelling as a foot passenger. You’ll enjoy a range of onboard facilities during a vehicle ferry crossing, including our premium Signature Lounge, pet-friendly lounge, licensed bar, café and retail shop plus outdoor sun decks.Aside from getting to the Isle of Wight, we also offer a range of fantastic IOW holiday and accommodation deals, alongside discounted attraction and event tickets plus a host of special offers for the perfect trip to the Island. All that’s left to do now is explore what you could enjoy during a visit and book your ferry to the Isle of Wight."

# make lowercase
#text <- tolower(text)

text <- data.frame(text=text)
file <- "text found on https://www.redfunnel.co.uk"

#file <- "text.csv"
#text <- read.csv(file)

#text <- text$reviewText

#text <- text[1:250]

#colnames(text) <- "text"

split_by_sentence <- "Yes" 
if (split_by_sentence == "Yes"){
  texts <- strsplit(text$text, '\\.(?=\\s|$)', perl = TRUE) # positive lookahead to split only where white space follows or end of string, PERL TRUE for PCRE support
  text_df <- data.frame(text=texts)
  colnames(text_df) <- c("text")
  # add fullstop to end of string
  text_df$text <- paste0(text_df$text, ".")
} else{
text_df <- text
}

text <- text_df$text

# filter out blank or 1 nchar values
text_df <- text_df |>
  filter(text != "" & nchar(text) > 1)

# make numbers and letters only
#text <- gsub("([^a-zA-Z0-9])+", " ", text_df$text)
#text_df$text <- gsub("([^a-zA-Z0-9])+", " ", text_df$text)

type <- "Term" # specify Document or Term for data types

###########################################################################
# get count of words, to display on bar charts
# remove punctuation

one_word <- VCorpus(VectorSource(text))

if (type == "Document"){
matrix <- DocumentTermMatrix(one_word, control=list(removePunctuation=TRUE,stopwords=TRUE)) # or DocumentTermMatrix
} else{
  matrix <- TermDocumentMatrix(one_word, control=list(removePunctuation=TRUE,stopwords=TRUE)) # or DocumentTermMatrix
}

counts <- colSums(as.matrix(matrix))
words <- matrix$Terms

m1 <- as.matrix(matrix)

# Sum all columns(words) to get frequency
words_frequency <- colSums(as.matrix(matrix))

#run frequencies
word.freq <- sort(rowSums(m1), decreasing=T)

#convert matrix to dataframe
frequencies<-as.data.frame(as.table(word.freq))

frequencies1 <- data.frame(lapply(frequencies, as.character), stringsAsFactors=FALSE)
frequencies1$type <- "unigram\n one word"

###########################################################################
# get count of 2 word phrases
BigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

if (type == "Document"){
tdm <- DocumentTermMatrix(one_word, control = list(tokenize = BigramTokenizer))
} else{
  tdm <- TermDocumentMatrix(one_word, control = list(tokenize = BigramTokenizer))
}


m2 <- as.matrix(tdm)

# Sum all columns(words) to get frequency
words_frequency_2 <- colSums(as.matrix(tdm))

#run frequencies
word.freq.2 <- sort(rowSums(m2), decreasing=T)

#convert matrix to dataframe
frequencies_2<-as.data.frame(as.table(word.freq.2))

frequencies2 <- data.frame(lapply(frequencies_2, as.character), stringsAsFactors=FALSE)
frequencies2$type <- "bigram\n two words"

###########################################################################
# get count of 3 word phrases
TrigramTokenizer <-
  function(x)
    unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE)

if (type == "Document"){
tdm <- DocumentTermMatrix(one_word, control = list(tokenize = TrigramTokenizer))
} else{
  tdm <- TermDocumentMatrix(one_word, control = list(tokenize = TrigramTokenizer))
}


m3 <- as.matrix(tdm)

# Sum all columns(words) to get frequency
words_frequency_3 <- colSums(as.matrix(tdm))

#run frequencies
word.freq.3 <- sort(rowSums(m3), decreasing=T)

#convert matrix to dataframe
frequencies_3<-as.data.frame(as.table(word.freq.3))

frequencies3 <- data.frame(lapply(frequencies_3, as.character), stringsAsFactors=FALSE)
frequencies3$type <- "trigram\n three words"

###########################################################################
# get sentence sentiment, and display in charts
# R VADER compound score
#vader_score <- get_vader(text)
vader_score <- vader_df(text)
vader_score <- select(vader_score,text,compound)#vader_score$compound

score_type <- ifelse(vader_score$compound < -0.25, "Negative",ifelse(vader_score$compound > 0.25,"Positive","Neutral"))

score_type <- data.frame(score_type)
colnames(score_type) <- "sentiment"

score_type_tot <- score_type %>%
  group_by(sentiment) %>%
  summarise(count=n())

senti_table <- cbind(vader_score,score_type)

###########################################################################
# get word sentiment, and display in charts
# R VADER compound score
#vader_score <- get_vader(text)
vader_score_word <- vader_df(frequencies$Var1)
vader_score_word <- select(vader_score_word,text,compound)#vader_score$compound

score_type_word <- ifelse(vader_score_word$compound < -0.25, "Negative",ifelse(vader_score_word$compound > 0.25,"Positive","Neutral"))

score_type_word <- data.frame(score_type_word)
colnames(score_type_word) <- "sentiment"

score_type_tot_word <- score_type_word %>%
  group_by(sentiment) %>%
  summarise(count=n())

senti_table_word <- cbind(vader_score_word,score_type_word,frequencies$Freq)

# for comparison cloud
senti_table_word <- senti_table_word %>%
  acast(text ~ sentiment, value.var = "frequencies$Freq", fill = 0)

###########################################################################
# get bigram sentiment, and display in charts
# R VADER compound score
#vader_score <- get_vader(text)
vader_score_bigram <- vader_df(frequencies2$Var1)
vader_score_bigram <- select(vader_score_bigram,text,compound)#vader_score$compound

# make text a factor
vader_score_bigram$text <- as.factor(vader_score_bigram$text)

score_type_bigram <- ifelse(vader_score_bigram$compound < -0.25, "Negative",ifelse(vader_score_bigram$compound > 0.25,"Positive","Neutral"))

score_type_bigram <- data.frame(score_type_bigram)
colnames(score_type_bigram) <- "sentiment"

score_type_tot_bigram <- score_type_bigram %>%
  group_by(sentiment) %>%
  summarise(count=n())

senti_table_bigram <- cbind(vader_score_bigram,score_type_bigram,frequencies2$Freq)

# make numeric
senti_table_bigram$`frequencies2$Freq` <- as.numeric(senti_table_bigram$`frequencies2$Freq`)

# for comparison cloud
senti_table_bigram <- senti_table_bigram %>%
  acast(text ~ sentiment, value.var = "frequencies2$Freq", fill = 0)

###########################################################################
# get topic modelling (LDA)
# tidymodels
topic_num <- 4

topics_matrix <- DocumentTermMatrix(one_word, control=list(removePunctuation=TRUE,stopwords=TRUE))

topics <- LDA(topics_matrix, k = topic_num, control = list(seed = 1234))

# tidy this
topics <- tidy(topics)

# sort by topic and beta descending, get top 10
topic_df <- data.frame()
for (i in (1:topic_num)){
  topic1 <- topics %>%
    filter(topic == i)
  
  topic1 <- topic1[order(topic1$beta, decreasing = TRUE),]
  
  topic1 <- topic1[1:10,]
  
  # generate estimated topic titles
  topic1$title <- paste(topic1$term[1], topic1$term[2], topic1$term[3], sep=" ")
  
  topic_df <- rbind(topic_df,topic1)
}

###########################################################################
# summarise text (extractive summarisation)
#LEXRANK THE TEXT FOR N NUMBER OF SUMMARIES
top_sentences <- lexRank(text, n=3)

###########################################################################
# put text in table, and allow searching by word/phrase (and highlight?) DT
datatable(top_sentences, extensions = 'Buttons', options = list(
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
  ))

# get wordcloud
#wordcloud2(frequencies)
# set colouring
#pal <- brewer.pal(9,"BuGn")

###########################################################################

# word associations for top 2 words
top_words_asso <- frequencies1[1:2,]
top_words_asso <- top_words_asso$Var1


word1_asso <- data.frame(findAssocs(matrix, top_words_asso[1], 0.95)) # find terms correlated with "room" 

word1_asso <- word1_asso %>%
  mutate(word = rownames(word1_asso))

word1_asso_graph <- word1_asso[1:10,] %>%
  ggplot(aes(x=reorder(word,word1_asso[1:10,1]),y=word1_asso[1:10,1])) + geom_point(size=4) + 
  coord_flip() + ylab('Correlation') + xlab('Term')

###########################################################################

# tf-idf for all unigram (single) words
tfidf_words <- text_df %>%
  mutate(document=file)

# unnest_tokens
tfidf_words <- tfidf_words %>%
  unnest_tokens(word, text) %>%
  count(document, word, sort = TRUE)

# compare to total
total_words <- tfidf_words %>%
  group_by(document) %>%
  summarize(total = sum(n))

tfidf_words <- left_join(tfidf_words, total_words)

tfidf_words <- tfidf_words %>%
  bind_tf_idf(word, document, n)

###########################################################################

# tf-idf for all bigram (two) words
tfidf_bigram <- text_df %>%
  mutate(document=file)

# unnest_tokens
tfidf_bigram <- tfidf_bigram %>%
  unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  filter(!is.na(bigram)) %>%
  count(document, bigram, sort = TRUE)

# compare to total
total_bigram <- tfidf_bigram %>%
  group_by(document) %>%
  summarize(total = sum(n))

tfidf_bigram <- left_join(tfidf_bigram, total_bigram)

tfidf_bigram <- tfidf_bigram %>%
  bind_tf_idf(bigram, document, n)

################################################################################

# get bigram as separate words and get count of these
bigrams_separated <- tfidf_bigram %>%
  separate(bigram, c("word1", "word2"), sep = " ")


################################################################################

# bigram network graph
bigram_graph <- select(bigrams_separated, word1, word2, n) %>%
  filter(n > 1) %>%
  graph_from_data_frame()

```

```{r functions, include=FALSE, echo=FALSE}

r_nchar_histogram_function <- function(df,x_df){
  tt <- ggplot(df,aes(x=x_df))+
    geom_histogram(binwidth = 1, alpha = 0.8, fill="#CD2456")+
    theme_classic()+
    labs(x="Number of characters per paragraph",y="Number of paragraphs")
  return (tt)
}

####################################################################################################

r_bar_graph_function <- function(df,x_df,y_df,xlab,ylab,title){
  tt <- ggplot(df,aes(x=reorder(x_df, -y_df),y=y_df))+
    geom_bar(aes(fill="#CD2456"),position="dodge",stat="identity",width=0.5)+
    geom_text(aes(label=paste(round(y_df,0))),position=position_dodge(width=.9),vjust=-0.5,size=3.5)+
    scale_fill_manual(values=alpha(c("#CD2456","#14022E")),name="xyz")+
    theme_classic()+
    theme(legend.position="none")+
    labs(x=xlab,y=ylab,title=title)
  return (tt)
}

####################################################################################################

r_bar_graph_function_flip <- function(df,x_df,y_df,xlab,ylab,title){
  tt <- ggplot(df,aes(x=reorder(x_df, y_df),y=y_df))+
    geom_bar(aes(fill="#CD2456"),position="dodge",stat="identity",width=0.5)+
    geom_text(aes(label=paste(round(y_df,0))),position=position_dodge(width=.9),vjust=0.5, hjust=-1.0, size=3.5)+
    scale_fill_manual(values=alpha(c("#CD2456","#14022E")),name="xyz")+
    coord_flip()+
    theme_classic()+
    theme(legend.position="none")+
    labs(x=xlab,y=ylab,title=title)
  return (tt)
}

####################################################################################################

r_scatter_plot_function <- function(df,x_df,y_df,xlab,ylab,title){
  tt <- ggplot(df,aes(x=x_df,y=y_df, color=y_df))+
    #geom_point(position="identity",stat="identity")+
    geom_jitter()+
    #scale_x_continuous(breaks=0:max(y_df))+
    coord_flip()+
    #scale_y_continuous(breaks=0:max(x_df))+
    theme_classic()+
    scale_color_gradient(low="blue", high="red")+
    #scale_color_brewer(palette="YlGn")+
    #scale_color_manual(values = c("blue","red"))+
    #theme(legend.position="none")+
    labs(x=xlab,y=ylab,title=title, color="Count")
  return (tt)
}

####################################################################################################

word_asso_graph_function <- function(word){
  word1 <- data.frame(findAssocs(matrix, word, 0.1)) # find terms correlated with "room" 

word1 <- word1 %>%
  mutate(word = rownames(word1))

word1_asso_graph <- word1[1:10,] %>%
  ggplot(aes(x=reorder(word,word1[1:10,1]),y=word1[1:10,1])) + geom_point(size=4,color="#CD2456") + 
  coord_flip()+
  #scale_fill_manual("#CD2456")+
  theme_classic()+
  ylim(min(word1[1:10,1]),1)+
  labs(x="Word",y="Correlation score",title=paste("Top 10 words associated with ",word,sep=""))
return(word1_asso_graph)
}

####################################################################################################

tfidf_graph <- function(df){
  tt <- ggplot(df, aes(n/total, fill = document)) +
  geom_histogram(show.legend = FALSE) +
  scale_fill_manual(values = "#cd2456")+
  #xlim(NA, 0.0009) +
  facet_wrap(~document, ncol = 2, scales = "free_y")+
  theme_classic()
return(tt)
}

####################################################################################################

linked_words_graph <- function(df, word, ylab){
  tt <- ggplot(df, aes(n * compound, word2, fill = n * compound > 0)) +
  geom_col(show.legend = FALSE) +
  scale_fill_manual(values=alpha(c("#CD2456","#14022E")))+
  theme_classic()+
  labs(x = "Sentiment value * number of occurrences",
       y = paste0(ylab, " ",word))
return(tt)
}

####################################################################################################

bigram_network_graph <- function(df){
  tt <- ggraph(df, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)+
  theme_void()
return(tt)
}

####################################################################################################

topic_models_graph <- function(){
  tt1 <- ggplot(topic_df, aes(reorder(topic_df$term,topic_df$beta),topic_df$beta,fill=topic_df$title))+
    geom_bar(position="dodge",stat="identity",width=0.5)+
    #geom_text(aes(label=paste(round(topic_df$beta,0))),position=position_dodge(width=.9),vjust=-0.5,size=3.5)+
    #scale_fill_manual(values=alpha(c("#CD2456","14022E")),name="xyz")+
    coord_flip()+
    facet_wrap(~topic_df$title, scales = "free")+
    theme_classic()+
    theme(legend.position="none")+
    labs(x="",y="",title="")
  return(tt1)
}
```

# Word text analysis

For the text from `r file` some analysis has been completed. This is shown below.

# Table of words used

Below is the table of words and associated counts.
```{r table_words, echo=FALSE}
datatable(select(frequencies1,-type), extensions = 'Buttons', options = list(
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
  ))
```

# Number of characters per sentence/paragraph

```{r hist1, echo=FALSE}
dat_to_use <- text_df
r_nchar_histogram_function(dat_to_use,nchar(dat_to_use$`text`))

```

# Count of words - top 10

The top 10 words found by their number of occurrences within the text is shown below. There are `r nrow(frequencies)` unique words used.

```{r graph1, echo=FALSE}
dat_to_use <- frequencies[1:10,]
r_bar_graph_function(dat_to_use,dat_to_use$`Var1`,dat_to_use$`Freq`,"Word","Count","Top 10 words")

```

# Wordcloud

A wordcloud of the text is provided below.

```{r wc1, echo=FALSE, message=FALSE, warning=FALSE}
# get wordcloud
wordcloud(words = frequencies$Var1, frequencies$Freq, min.freq = 1,           
          max.words=nrow(frequencies), random.order=FALSE, rot.per=0.35, random.color = TRUE, colors=brewer.pal(8, "Dark2"))
```

# Comparison wordcloud

A wordcloud of the sentiment of text is provided below.

```{r swc1, echo=FALSE, message=FALSE, warning=FALSE}
# get sentiment wordcloud

# for comparison cloud
#senti_table_word <- senti_table_word %>%
#  acast(text ~ sentiment, value.var = "frequencies$Freq", fill = 0)

comparison.cloud(term.matrix = senti_table_word, colors=c("gray20","gray50", "grey80"))

#wordcloud(words = frequencies$Var1, frequencies$Freq, min.freq = 1,           
#          max.words=nrow(frequencies), random.order=FALSE, rot.per=0.35, random.color = TRUE, colors=brewer.pal(8, "Dark2"))
```

# TF-IDF of unigram (single) words

This graph shows the tf-idf (term-frequency inverse-document frequency) of single words. Words that repeat often have a lower score, and a higher count of words.

```{r tfidfgraph1, echo=FALSE}
dat_to_use <- tfidf_words
tfidf_graph(dat_to_use)

```

# Count of two word phrases - top 5

The top 5 2-word phrases, by the number of times they appear are below. There are `r nrow(frequencies2)` two word phrases.

```{r graph2, echo=FALSE}
dat_to_use <- frequencies_2[1:10,]

# split space as new line
dat_to_use$`Var1` <- gsub(" ", "\n", dat_to_use$`Var1`)

r_bar_graph_function(dat_to_use,dat_to_use$`Var1`,dat_to_use$`Freq`,"Phrase","Count","Top 5 two word phrases")
r_bar_graph_function_flip(dat_to_use,dat_to_use$`Var1`,dat_to_use$`Freq`,"Phrase","Count","Top 5 two word phrases")

```

# Count of unique phrases by type
It may not look like it, but there are `r sum(as.numeric(frequencies1$Freq), as.numeric(frequencies2$Freq), as.numeric(frequencies3$Freq))` datapoints on this graph.
<br>
Of these:
<br>
Unigram is: `r sum(as.numeric(frequencies1$Freq))` which is `r round(sum(as.numeric(frequencies1$Freq))/sum(as.numeric(frequencies1$Freq), as.numeric(frequencies2$Freq), as.numeric(frequencies3$Freq)), 2)*100`% of the total
<br>
Bigram is: `r sum(as.numeric(frequencies2$Freq))` which is `r round(sum(as.numeric(frequencies2$Freq))/sum(as.numeric(frequencies1$Freq), as.numeric(frequencies2$Freq), as.numeric(frequencies3$Freq)), 2)*100`% of the total
<br>
Trigram is: `r sum(as.numeric(frequencies3$Freq))` which is `r round(sum(as.numeric(frequencies3$Freq))/sum(as.numeric(frequencies1$Freq), as.numeric(frequencies2$Freq), as.numeric(frequencies3$Freq)), 2)*100`% of the total
```{r unqphrase, echo=FALSE, message=FALSE, warning=FALSE}

# join all types together
# frequencies1, frequencies2, frequencies3

freqs <- rbind(frequencies1, frequencies2, frequencies3)

level_order <- factor(freqs$type,level=c("trigram\n three words", "bigram\n two words", "unigram\n one word"))

r_scatter_plot_function(freqs,level_order,as.numeric(freqs$`Freq`),"Type","Count","Count of unique occurrences")
```

# Wordcloud - two word phrases

A wordcloud of the text is provided below.

```{r wc2, echo=FALSE, message=FALSE, warning=FALSE}
# get wordcloud
#plot.new()
#frame()

# get wordcloud
wordcloud(words = frequencies_2$Var1, frequencies_2$Freq, min.freq = 1,           
          max.words=nrow(frequencies_2), random.order=FALSE, rot.per=0.35, random.color = TRUE, colors=brewer.pal(8, "Dark2"))
```

# Comparison two-phrase wordcloud

A wordcloud of the sentiment of text is provided below.

```{r swc2, echo=FALSE, message=FALSE, warning=FALSE}
# get sentiment wordcloud

# for comparison cloud
#senti_table_word <- senti_table_word %>%
#  acast(text ~ sentiment, value.var = "frequencies$Freq", fill = 0)

comparison.cloud(term.matrix = senti_table_bigram, colors=c("gray20","gray50", "gray80"))

#wordcloud(words = frequencies$Var1, frequencies$Freq, min.freq = 1,           
#          max.words=nrow(frequencies), random.order=FALSE, rot.per=0.35, random.color = TRUE, colors=brewer.pal(8, "Dark2"))
```

# TF-IDF of bigram (two) words

This graph shows the tf-idf (term-frequency inverse-document frequency) of single words. Words that repeat often have a lower score, and a higher count of words.

```{r tfidfgraph2, echo=FALSE}
dat_to_use <- tfidf_bigram
tfidf_graph(dat_to_use)

```

# Network graph of bigrams that occur more than once

This network graph shows the linkages used between two word phrases which occur more than once in the text.

```{r bigramntwrk, echo=FALSE}
dat_to_use <- bigram_graph
bigram_network_graph(dat_to_use)

```

# Sentiment of text

The sentiment types of the text are shown below. There are `r length(unique(score_type$sentiment))` types of sentiment and the sentiment types are: `r unique(score_type$sentiment)`.

```{r sentiment, echo=FALSE}
r_bar_graph_function(score_type_tot,score_type_tot$`sentiment`,score_type_tot$`count`,"Sentiment","Count","Count of sentiment")
```

Below is the table of text and associated sentiment.
```{r table_sentiment, echo=FALSE}
datatable(senti_table, extensions = 'Buttons', options = list(
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
  ))
```

# Topic modelling

Estimated topics from the text, highest 10 scoring words on graph. There are `r topic_num` topics chosen.

```{r tm, echo=FALSE, message=FALSE, warning=FALSE}
topic_models_graph()
```

# Top 3 sentences

The top 3 scoring (summary extraction) sentences are below.

```{r top3, echo=FALSE}
datatable(top_sentences, extensions = 'Buttons', options = list(
    dom = 'Bfrtip',
    buttons = c('copy', 'csv', 'excel', 'pdf', 'print')
  ))
```

# Word associations for top 2 words

## Word 1 - `r top_words_asso[1]`

Word 1 is `r top_words_asso[1]` and the top 10 words associated with it are below

```{r word1, echo=FALSE}
word1_choice <- top_words_asso[1]
word_asso_graph_function(word1_choice)
```

## Word 2 - `r top_words_asso[2]`

Word 2 is `r top_words_asso[2]` and the top 10 words associated with it are below

```{r word2, echo=FALSE}
word2_choice <- top_words_asso[2]
word_asso_graph_function(word2_choice)
```

# Get value and sentiment of words preceded by these top 2 words

## Word 1 - `r top_words_asso[1]`

```{r asso_words_value1, echo=FALSE}

# get words linked to this word *** word 1
asso_words_value <- bigrams_separated %>%
  filter(word1 == top_words_asso[1]) # filter matches to only include this word as word1

# then, get VADER sentiment for word2
vader_score_bigram_test <- vader_df(asso_words_value$word2)
vader_score_bigram_test <- select(vader_score_bigram_test,text,compound)#vader_score$compound

# replace 0 values with 0.01
vader_score_bigram_test$compound <- ifelse(vader_score_bigram_test$compound == 0,0.01, vader_score_bigram_test$compound)

# join data
asso_words_value <- cbind(asso_words_value, vader_score_bigram_test)

asso_words_value_graph <- asso_words_value %>%
  mutate(contribution = n * compound) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution))

# graph this
linked_words_graph(asso_words_value_graph, top_words_asso[1], "Words preceded by")

```

## Word 2 - `r top_words_asso[2]`

```{r asso_words_value2, echo=FALSE}

# get words linked to this word
asso_words_value <- bigrams_separated %>%
  filter(word1 == top_words_asso[2]) # filter matches to only include this word as word1

# then, get VADER sentiment for word2
vader_score_bigram_test <- vader_df(asso_words_value$word2)
vader_score_bigram_test <- select(vader_score_bigram_test,text,compound)#vader_score$compound

# replace 0 values with 0.01
vader_score_bigram_test$compound <- ifelse(vader_score_bigram_test$compound == 0,0.01, vader_score_bigram_test$compound)

# join data
asso_words_value <- cbind(asso_words_value, vader_score_bigram_test)

asso_words_value_graph <- asso_words_value %>%
  mutate(contribution = n * compound) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word2, contribution))

# graph this
linked_words_graph(asso_words_value_graph, top_words_asso[2], "Words preceded by")

```

# Get value and sentiment of words followed by these top 2 words

## Word 1 - `r top_words_asso[1]`

```{r asso_words1_value1, echo=FALSE}

# get words linked to this word *** word 1
asso_words_value <- bigrams_separated %>%
  filter(word2 == top_words_asso[1]) # filter matches to only include this word as word1

# then, get VADER sentiment for word2
vader_score_bigram_test <- vader_df(asso_words_value$word1)
vader_score_bigram_test <- select(vader_score_bigram_test,text,compound)#vader_score$compound

# replace 0 values with 0.01
vader_score_bigram_test$compound <- ifelse(vader_score_bigram_test$compound == 0,0.01, vader_score_bigram_test$compound)

# join data
asso_words_value <- cbind(asso_words_value, vader_score_bigram_test)

asso_words_value_graph <- asso_words_value %>%
  mutate(contribution = n * compound) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word1, contribution))

# graph this
linked_words_graph(asso_words_value_graph, top_words_asso[1], "Words followed by")

```

## Word 2 - `r top_words_asso[2]`

```{r asso_words1_value2, echo=FALSE}

# get words linked to this word
asso_words_value <- bigrams_separated %>%
  filter(word2 == top_words_asso[2]) # filter matches to only include this word as word1

# then, get VADER sentiment for word2
vader_score_bigram_test <- vader_df(asso_words_value$word1)
vader_score_bigram_test <- select(vader_score_bigram_test,text,compound)#vader_score$compound

# replace 0 values with 0.01
vader_score_bigram_test$compound <- ifelse(vader_score_bigram_test$compound == 0,0.01, vader_score_bigram_test$compound)

# join data
asso_words_value <- cbind(asso_words_value, vader_score_bigram_test)

asso_words_value_graph <- asso_words_value %>%
  mutate(contribution = n * compound) %>%
  arrange(desc(abs(contribution))) %>%
  head(20) %>%
  mutate(word2 = reorder(word1, contribution))

# graph this
linked_words_graph(asso_words_value_graph, top_words_asso[2], "Words followed by")

```